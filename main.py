# ==========================
# ETAPA 1: INIÈšIALIZARE SPARK
# ==========================

# ImportÄƒm biblioteca SparkSession din pyspark.sql
# SparkSession este punctul de pornire pentru toate aplicaÈ›iile Spark.
from pyspark.sql import SparkSession

# CreÄƒm o sesiune Spark cu un nume descriptiv.
# getOrCreate() Ã®nseamnÄƒ: â€creeazÄƒ o nouÄƒ sesiune dacÄƒ nu existÄƒ dejaâ€.
spark = SparkSession.builder \
    .appName("ACS2013_LearningAnalytics") \
    .getOrCreate()

# AfiÈ™Äƒm versiunea Spark pentru a verifica cÄƒ totul funcÈ›ioneazÄƒ.
print("Spark version:", spark.version)

# Ca test, putem crea un mic DataFrame local pentru a ne asigura cÄƒ Spark funcÈ›ioneazÄƒ corect.
data_test = [(1, "ok"), (2, "merge")]
columns = ["id", "status"]

# CreÄƒm DataFrame-ul test.
df_test = spark.createDataFrame(data_test, columns)

# AfiÈ™Äƒm schema pentru a verifica tipurile de date.
df_test.printSchema()

# AfiÈ™Äƒm conÈ›inutul DataFrame-ului.
df_test.show()

# ================================
# ETAPA 2: ÃNCÄ‚RCAREA È˜I EXPLORAREA DATELOR
# ================================

from pyspark.sql import SparkSession
from pyspark.sql import functions as F  # pentru agregÄƒri È™i calcule
import os

# CreÄƒm o sesiune Spark (dacÄƒ nu existÄƒ deja)
spark = SparkSession.builder.appName("ACS2013_LearningAnalytics").getOrCreate()

# -----------------------------
# 1ï¸âƒ£ SetÄƒm calea cÄƒtre fiÈ™ierul CSV
# -----------------------------
# Presupunem cÄƒ fiÈ™ierul este Ã®n folderul proiectului, Ã®n subdirectorul 'data'
# Exemplu: proiectul tÄƒu are structura:
#  proiect/
#   â”œâ”€â”€ main.py
#   â”œâ”€â”€ data
#   â”œâ”€â”€â”€â”€â”€â”€ ACS2013.csv

csv_path = os.path.join("", "data/ACS2013.csv")

# -----------------------------
# 2ï¸âƒ£ Citim fiÈ™ierul CSV Ã®n Spark
# -----------------------------
# - header=True  â†’ prima linie are numele coloanelor
# - inferSchema=True â†’ Spark detecteazÄƒ automat tipurile de date
# âš ï¸ Prima citire poate dura cÃ¢teva secunde, fiind un fiÈ™ier mare.
df = spark.read.csv(csv_path, header=True, inferSchema=True)

# -----------------------------
# 3ï¸âƒ£ VerificÄƒm schema detectatÄƒ
# -----------------------------
# df.printSchema()  # afiÈ™eazÄƒ tipurile de date ale coloanelor

# -----------------------------
# 4ï¸âƒ£ VizualizÄƒm primele 5 rÃ¢nduri
# -----------------------------
# df.show(5, truncate=False)

# -----------------------------
# 5ï¸âƒ£ Statistici rapide
# -----------------------------
# numÄƒr de Ã®nregistrÄƒri
print(f"NumÄƒr total de Ã®nregistrÄƒri: {df.count():,}")

# nume coloane
print(f"NumÄƒr coloane: {len(df.columns)}")
print("Primele 10 coloane:", df.columns[:10])

# -----------------------------
# 6ï¸âƒ£ SelectÄƒm cÃ¢teva coloane-cheie
# -----------------------------
# Pentru analiza veniturilor folosim cÃ¢teva coloane relevante
cols_cheie = ["AGEP", "EDUC", "SEX", "RACE", "INCOME"]

# FiltrÄƒm doar coloanele care existÄƒ efectiv Ã®n dataset (Ã®n unele versiuni pot lipsi)
cols_existente = [c for c in cols_cheie if c in df.columns]

# AfiÈ™Äƒm un eÈ™antion din aceste coloane
# df.select(*cols_existente).show(10, truncate=False)

# -----------------------------
# 7ï¸âƒ£ NumÄƒrÄƒm valorile lipsÄƒ pe fiecare coloanÄƒ-cheie
# -----------------------------
null_counts = df.select([
    F.sum(F.col(c).isNull().cast("int")).alias(c)
    for c in cols_existente
]).collect()[0].asDict()

print("NumÄƒr de valori lipsÄƒ / coloanÄƒ:")
for col, val in null_counts.items():
    print(f" - {col}: {val}")

# -----------------------------
# 8ï¸âƒ£ Statistici descriptive de bazÄƒ
# -----------------------------


# AfiÈ™Äƒm toate coloanele disponibile Ã®n datasetul citit
print("Total coloane:", len(df.columns))

# CalculÄƒm media, min, max È™i deviaÈ›ia standard pentru vÃ¢rstÄƒ È™i venit
df.select("AGEP", "PINCP").describe().show()

# -----------------------------
# 9ï¸âƒ£ VerificÄƒm distribuÈ›ia veniturilor (opÈ›ional)
# -----------------------------
# GrupÄƒm pe intervale de venit pentru o vedere rapidÄƒ
if "PINCP" in df.columns:
    df.groupBy().agg(
        F.avg("PINCP").alias("Venit_mediu"),
        F.max("PINCP").alias("Venit_maxim"),
        F.min("PINCP").alias("Venit_minim")
    ).show()

# ================================
# ETAPA 3: CURÄ‚ÈšARE È˜I TRANSFORMARE
# ================================

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.sql.types import DoubleType

# 1ï¸âƒ£ IniÈ›ializÄƒm sesiunea Spark (dacÄƒ nu existÄƒ deja)
spark = SparkSession.builder.appName("ACS2013_LearningAnalytics").getOrCreate()

# 2ï¸âƒ£ Citim fiÈ™ierul CSV (setul de populaÈ›ie)
csv_path = "data/ACS2013.csv"
df = spark.read.csv(csv_path, header=True, inferSchema=True)

# 3ï¸âƒ£ SelectÄƒm doar coloanele relevante pentru analiza veniturilor
# AGEP - vÃ¢rstÄƒ
# SCHL - nivel educaÈ›ional
# SEX - gen
# RAC1P - rasÄƒ
# PINCP - venit personal (target)
df = df.select("AGEP", "SCHL", "SEX", "RAC1P", "PINCP")

# 4ï¸âƒ£ EliminÄƒm rÃ¢ndurile care au valori lipsÄƒ (null) Ã®n oricare dintre coloanele de interes
df = df.dropna(subset=["AGEP", "SCHL", "SEX", "RAC1P", "PINCP"])

# 5ï¸âƒ£ FiltrÄƒm veniturile negative (unele coduri pot fi -1 sau valori invalide)
df = df.filter(df["PINCP"] > 0)

# 6ï¸âƒ£ Convertim coloanele numerice la tip Double (uneori inferSchema le pune ca IntegerType)
df = df.withColumn("AGEP", F.col("AGEP").cast(DoubleType())) \
       .withColumn("SCHL", F.col("SCHL").cast(DoubleType())) \
       .withColumn("SEX", F.col("SEX").cast(DoubleType())) \
       .withColumn("RAC1P", F.col("RAC1P").cast(DoubleType())) \
       .withColumn("PINCP", F.col("PINCP").cast(DoubleType()))

# 7ï¸âƒ£ VerificÄƒm rapid schema È™i cÃ¢teva rÃ¢nduri
df.printSchema()
df.show(5)

# 8ï¸âƒ£ CodificÄƒm variabilele categorice (SEX, RAC1P, SCHL)
# StringIndexer transformÄƒ valorile numerice/categorice Ã®n indecÈ™i 0,1,2,...
# (necesar pentru modelare MLlib)
indexer_sex = StringIndexer(inputCol="SEX", outputCol="SEX_idx")
indexer_race = StringIndexer(inputCol="RAC1P", outputCol="RAC1P_idx")
indexer_edu = StringIndexer(inputCol="SCHL", outputCol="SCHL_idx")

# AplicÄƒm transformÄƒrile
df = indexer_sex.fit(df).transform(df)
df = indexer_race.fit(df).transform(df)
df = indexer_edu.fit(df).transform(df)

# 9ï¸âƒ£ Construim vectorul de trÄƒsÄƒturi (features) pentru MLlib
# CombinÄƒm AGEP, SEX_idx, RAC1P_idx È™i SCHL_idx Ã®ntr-o singurÄƒ coloanÄƒ "features"
assembler = VectorAssembler(
    inputCols=["AGEP", "SEX_idx", "RAC1P_idx", "SCHL_idx"],
    outputCol="features"
)

df = assembler.transform(df)

# ğŸ”Ÿ VerificÄƒm rezultatul final
df.select("AGEP", "SEX_idx", "RAC1P_idx", "SCHL_idx", "PINCP", "features").show(5, truncate=False)

# 11ï¸âƒ£ (OpÈ›ional) AfiÈ™Äƒm statistici rapide pentru veniturile curÄƒÈ›ate
df.select(F.mean("PINCP").alias("Venit mediu"),
          F.max("PINCP").alias("Venit maxim"),
          F.min("PINCP").alias("Venit minim")).show()

# ================================
# ETAPA 4: MODELARE È˜I EVALUARE MLLIB
# ================================

from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# 1ï¸âƒ£ IniÈ›ializÄƒm Spark (dacÄƒ nu e deja pornit)
spark = SparkSession.builder.appName("ACS2013_LearningAnalytics").getOrCreate()

# 2ï¸âƒ£ Presupunem cÄƒ avem deja DataFrame-ul curÄƒÈ›at din Etapa 3: `df`
# Cu coloanele: ["AGEP", "SCHL", "SEX", "RAC1P", "PINCP", "features"]

# 3ï¸âƒ£ ÃmpÄƒrÈ›im datele Ã®n seturi de antrenare È™i test
# - 80% pentru antrenare
# - 20% pentru testare
train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)

print(f"Train set: {train_data.count():,} rÃ¢nduri")
print(f"Test set: {test_data.count():,} rÃ¢nduri")

# 4ï¸âƒ£ Definim evaluatorul comun pentru toate modelele
# Vom folosi metricile RMSE È™i RÂ²
evaluator_rmse = RegressionEvaluator(
    labelCol="PINCP", predictionCol="prediction", metricName="rmse"
)
evaluator_r2 = RegressionEvaluator(
    labelCol="PINCP", predictionCol="prediction", metricName="r2"
)

# =======================================================
# ğŸŸ¢ MODEL 1: REGRESIE LINIARÄ‚
# =======================================================
print("\n=== Model 1: Regresie LiniarÄƒ ===")

# IniÈ›ializÄƒm modelul de regresie liniarÄƒ
lr = LinearRegression(featuresCol="features", labelCol="PINCP", maxIter=100)

# AntrenÄƒm modelul pe datele de training
lr_model = lr.fit(train_data)

# AplicÄƒm modelul pe datele de test
lr_predictions = lr_model.transform(test_data)

# CalculÄƒm metricile de performanÈ›Äƒ
rmse_lr = evaluator_rmse.evaluate(lr_predictions)
r2_lr = evaluator_r2.evaluate(lr_predictions)

print(f"RMSE (Regresie LiniarÄƒ): {rmse_lr:.2f}")
print(f"RÂ² (Regresie LiniarÄƒ): {r2_lr:.4f}")

# =======================================================
# ğŸŸ  MODEL 2: ARBORE DE DECIZIE
# =======================================================
print("\n=== Model 2: Arbore de Decizie ===")

# IniÈ›ializÄƒm modelul
dt = DecisionTreeRegressor(featuresCol="features", labelCol="PINCP", maxDepth=10)

# AntrenÄƒm modelul
dt_model = dt.fit(train_data)

# PredicÈ›ii
dt_predictions = dt_model.transform(test_data)

# Evaluare
rmse_dt = evaluator_rmse.evaluate(dt_predictions)
r2_dt = evaluator_r2.evaluate(dt_predictions)

print(f"RMSE (Arbore de Decizie): {rmse_dt:.2f}")
print(f"RÂ² (Arbore de Decizie): {r2_dt:.4f}")

# =======================================================
# ğŸ”µ MODEL 3: PÄ‚DURE ALEATORIE (Random Forest)
# =======================================================
print("\n=== Model 3: PÄƒdure Aleatorie ===")

# IniÈ›ializÄƒm modelul de tip ensemble
rf = RandomForestRegressor(
    featuresCol="features",
    labelCol="PINCP",
    numTrees=20,       # numÄƒr de arbori
    maxDepth=10,       # adÃ¢ncime maximÄƒ
    seed=42
)

# AntrenÄƒm modelul
rf_model = rf.fit(train_data)

# PredicÈ›ii
rf_predictions = rf_model.transform(test_data)

# Evaluare
rmse_rf = evaluator_rmse.evaluate(rf_predictions)
r2_rf = evaluator_r2.evaluate(rf_predictions)

print(f"RMSE (Random Forest): {rmse_rf:.2f}")
print(f"RÂ² (Random Forest): {r2_rf:.4f}")

# =======================================================
# ğŸ” COMPARAÈšIE FINALÄ‚
# =======================================================
print("\n=== Rezumat performanÈ›Äƒ modele ===")
print(f"{'Model':25s} | {'RMSE':>10s} | {'RÂ²':>10s}")
print("-" * 50)
print(f"{'Regresie LiniarÄƒ':25s} | {rmse_lr:10.2f} | {r2_lr:10.4f}")
print(f"{'Arbore de Decizie':25s} | {rmse_dt:10.2f} | {r2_dt:10.4f}")
print(f"{'PÄƒdure Aleatorie':25s} | {rmse_rf:10.2f} | {r2_rf:10.4f}")



# ================================
# ETAPA 5: VIZUALIZAREA È˜I SALVAREA GRAFICELOR
# ================================

import os
import matplotlib.pyplot as plt

# 1ï¸âƒ£ CreÄƒm folderul pentru rezultate (dacÄƒ nu existÄƒ)
results_dir = "results"
os.makedirs(results_dir, exist_ok=True)

# 2ï¸âƒ£ Definim datele pentru graficul comparativ
models = ["Regresie LiniarÄƒ", "Arbore de Decizie", "PÄƒdure Aleatorie"]
rmse_values = [rmse_lr, rmse_dt, rmse_rf]
r2_values = [r2_lr, r2_dt, r2_rf]

# =======================================================
# ğŸŸ¢ GRAFIC 1: Compararea RMSE Ã®ntre modele
# =======================================================
plt.figure(figsize=(8, 5))
bars = plt.bar(models, rmse_values, color=["#b2182b", "#f4a582", "#2166ac"])
plt.title("Compararea erorii RMSE Ã®ntre modele", fontsize=14)
plt.ylabel("RMSE", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# AdÄƒugÄƒm valorile numerice deasupra barelor
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f"{yval:.0f}",
             ha='center', va='bottom', fontsize=10, color="black")

# SalvÄƒm imaginea
rmse_img_path = os.path.join(results_dir, "comparatie_rmse.png")
plt.tight_layout()
plt.savefig(rmse_img_path)
plt.close()
print(f"âœ… Grafic RMSE salvat la: {rmse_img_path}")

# =======================================================
# ğŸŸ£ GRAFIC 2: Compararea RÂ² Ã®ntre modele
# =======================================================
plt.figure(figsize=(8, 5))
bars = plt.bar(models, r2_values, color=["#7fc97f", "#fdc086", "#beaed4"])
plt.title("Compararea scorului RÂ² Ã®ntre modele", fontsize=14)
plt.ylabel("RÂ²", fontsize=12)
plt.xlabel("Model", fontsize=12)
plt.ylim(0, 1)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# AdÄƒugÄƒm valorile numerice deasupra barelor
for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.02, f"{yval:.2f}",
             ha='center', va='bottom', fontsize=10, color="black")

# SalvÄƒm imaginea
r2_img_path = os.path.join(results_dir, "comparatie_r2.png")
plt.tight_layout()
plt.savefig(r2_img_path)
plt.close()
print(f"âœ… Grafic RÂ² salvat la: {r2_img_path}")

# =======================================================
# ğŸ”µ GRAFIC 3: Scatter plot predicÈ›ii vs valori reale (pentru Random Forest)
# =======================================================

# Pentru a face graficul, extragem un eÈ™antion mic din setul de test (altfel sunt prea multe puncte)
sample_df = rf_predictions.select("PINCP", "prediction").sample(fraction=0.001, seed=42)

# Convertim la Pandas pentru matplotlib
sample_pd = sample_df.toPandas()

plt.figure(figsize=(6, 6))
plt.scatter(sample_pd["PINCP"], sample_pd["prediction"], alpha=0.4, s=10, color="#377eb8")
plt.plot([0, sample_pd["PINCP"].max()], [0, sample_pd["PINCP"].max()], color="red", lw=2, label="Perfect match")
plt.title("PredicÈ›ii vs Valori reale (Random Forest)", fontsize=13)
plt.xlabel("Valori reale PINCP")
plt.ylabel("PredicÈ›ii PINCP")
plt.legend()
plt.grid(alpha=0.5)

# SalvÄƒm imaginea
scatter_img_path = os.path.join(results_dir, "scatter_pred_vs_real_rf.png")
plt.tight_layout()
plt.savefig(scatter_img_path)
plt.close()
print(f"âœ… Grafic scatter salvat la: {scatter_img_path}")

# =======================================================
# ğŸ” Raport sumar
# =======================================================
print("\n=== GRAFICE SALVATE ===")
print(f"- {rmse_img_path}")
print(f"- {r2_img_path}")
print(f"- {scatter_img_path}")

